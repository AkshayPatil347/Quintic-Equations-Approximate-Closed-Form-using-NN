{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927af7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shallower NN with trainable Chebyshev Activation Function - Deterministic Inpterpretation of NN - 39000 Parameters\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ChebyshevActivation(nn.Module):\n",
    "    def __init__(self, in_features, degree=15):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        # Learnable coefficients for each feature\n",
    "        self.coeffs = nn.Parameter(torch.randn(in_features, degree + 1, dtype=torch.float32))\n",
    "        \n",
    "        self._initialize_parameters()\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initialize coefficients with appropriate scheme for numerical stability\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Use smaller std for stability, considering polynomial degree\n",
    "            std = 0.01 / math.sqrt(self.degree + 1)\n",
    "            nn.init.normal_(self.coeffs, mean=0.0, std=std)\n",
    "            \n",
    "            # Ensure first coefficient (constant term) is reasonable\n",
    "            self.coeffs[:, 0].uniform_(-0.1, 0.1)\n",
    "\n",
    "    def _chebyshev_basis(self, x):\n",
    "        \"\"\"Compute Chebyshev polynomial basis functions with numerical stability\"\"\"\n",
    "        \n",
    "        # Clamp input to prevent extreme polynomial values\n",
    "        x = torch.clamp(x, -2.0, 2.0)\n",
    "        \n",
    "        # Compute polynomials using stable recurrence\n",
    "        T_list = [torch.ones_like(x)]\n",
    "        if self.degree >= 1:\n",
    "            T_list.append(x)\n",
    "            for k in range(1, self.degree):\n",
    "                Tk = 2 * x * T_list[k] - T_list[k-1]\n",
    "                # Clamp intermediate results to prevent overflow\n",
    "                Tk = torch.clamp(Tk, -10.0, 10.0)\n",
    "                T_list.append(Tk)\n",
    "        \n",
    "        return torch.stack(T_list, dim=-1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute Chebyshev basis\n",
    "        basis = self._chebyshev_basis(x)  # (batch_size, in_features, degree+1)\n",
    "        \n",
    "        # Apply coefficients\n",
    "        return torch.einsum('bfk,fk->bf', basis, self.coeffs)\n",
    "\n",
    "class PolynomialRootsModel(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dims=[64,128,128,64], output_dim=10, cheb_degree=10):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Create layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        self.layers.append(ChebyshevActivation(hidden_dims[0], degree=cheb_degree))\n",
    "        # self.layers.append(nn.BatchNorm1d(hidden_dims[0]))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            self.layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            self.layers.append(ChebyshevActivation(hidden_dims[i], degree=cheb_degree))\n",
    "            # self.layers.append(nn.BatchNorm1d(hidden_dims[i]))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def extract_complex_parts(complex_value):\n",
    "    \"\"\"Extract real and imaginary parts from complex number or string\"\"\"\n",
    "    try:\n",
    "        if isinstance(complex_value, str):\n",
    "            complex_value = complex_value.strip()\n",
    "            complex_num = complex(complex_value)\n",
    "        elif isinstance(complex_value, (int, float)):\n",
    "            complex_num = complex(complex_value)\n",
    "        else:\n",
    "            complex_num = complex(complex_value)\n",
    "        \n",
    "        return np.real(complex_num), np.imag(complex_num)\n",
    "    except (ValueError, TypeError):\n",
    "        print(f\"Warning: Could not parse complex number: {complex_value}\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "def load_and_preprocess_data(file_path, test_size=0.2, val_size=0.25, random_state=42):\n",
    "    \"\"\"Load data from CSV, preprocess it, and split into train/val/test sets\"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Dataset columns: {df.columns.tolist()}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Extract input features (polynomial coefficients)\n",
    "    input_cols = ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "    X = df[input_cols].values\n",
    "    \n",
    "    # Extract output features (roots)\n",
    "    y = np.zeros((len(df), 10))\n",
    "    \n",
    "    for i in range(5):\n",
    "        root_col = f'root_{i+1}'\n",
    "        if root_col in df.columns:\n",
    "            for j, complex_value in enumerate(df[root_col].values):\n",
    "                real_part, imag_part = extract_complex_parts(complex_value)\n",
    "                y[j, i] = real_part      # Real parts (columns 0-4)\n",
    "                y[j, i+5] = imag_part    # Imaginary parts (columns 5-9)\n",
    "    \n",
    "    print(f\"Input shape: {X.shape}\")\n",
    "    print(f\"Output shape: {y.shape}\")\n",
    "    \n",
    "    # Check for any NaN or infinite values\n",
    "    print(f\"NaN values in X: {np.isnan(X).sum()}\")\n",
    "    print(f\"NaN values in y: {np.isnan(y).sum()}\")\n",
    "    print(f\"Infinite values in X: {np.isinf(X).sum()}\")\n",
    "    print(f\"Infinite values in y: {np.isinf(y).sum()}\")\n",
    "    \n",
    "    # Clean data (remove NaN or infinite values)\n",
    "    valid_indices = ~(np.isnan(X).any(axis=1) | np.isnan(y).any(axis=1) | \n",
    "                      np.isinf(X).any(axis=1) | np.isinf(y).any(axis=1))\n",
    "    X = X[valid_indices]\n",
    "    y = y[valid_indices]\n",
    "    \n",
    "    print(f\"After cleaning - X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    \n",
    "    # Split and scale data\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_size, random_state=random_state)\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_val_scaled = scaler_X.transform(X_val)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    \n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    y_val_scaled = scaler_y.transform(y_val)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "    \n",
    "    print(\"Data preprocessing completed successfully!\")\n",
    "    \n",
    "    # Convert to PyTorch tensors and prepare DataLoaders\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train_scaled), torch.FloatTensor(y_train_scaled))\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val_scaled), torch.FloatTensor(y_val_scaled))\n",
    "    test_dataset = TensorDataset(torch.FloatTensor(X_test_scaled), torch.FloatTensor(y_test_scaled))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=512)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, scaler_X, scaler_y\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=1000, lr=0.01, patience=5, \n",
    "               save_path='best_polynomial_roots_model.pt'):\n",
    "    \"\"\"Train the PyTorch model with early stopping and learning rate reduction\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=7, verbose=True, min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss = criterion(outputs, targets)\n",
    "                running_val_loss += val_loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch+1}/{epochs} - '\n",
    "              f'Train Loss: {epoch_train_loss:.6f} - '\n",
    "              f'Val Loss: {epoch_val_loss:.6f}')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # # Early stopping check\n",
    "        # if epoch_val_loss < best_val_loss:\n",
    "        #     best_val_loss = epoch_val_loss\n",
    "        #     patience_counter = 0\n",
    "        #     # Save the best model\n",
    "        #     torch.save(model.state_dict(), save_path)\n",
    "        #     print(f\"Model saved at epoch {epoch+1} with validation loss: {best_val_loss:.6f}\")\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     if patience_counter >= patience:\n",
    "        #         print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        #         break\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict_polynomial_roots(model, coefficients, scaler_X, scaler_y):\n",
    "    \"\"\"\n",
    "    Predict roots for a polynomial given coefficients [a,b,c,d,e,f]\n",
    "    where polynomial is: a*x^5 + b*x^4 + c*x^3 + d*x^2 + e*x + f = 0\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure input is numpy array\n",
    "    coeffs = np.array(coefficients).reshape(1, -1)\n",
    "    \n",
    "    # Scale the input\n",
    "    coeffs_scaled = scaler_X.transform(coeffs)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    coeffs_tensor = torch.FloatTensor(coeffs_scaled).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        prediction_scaled = model(coeffs_tensor)\n",
    "    \n",
    "    # Convert back to numpy\n",
    "    prediction_scaled_np = prediction_scaled.cpu().numpy()\n",
    "    \n",
    "    # Inverse scale the output\n",
    "    prediction = scaler_y.inverse_transform(prediction_scaled_np)\n",
    "    \n",
    "    # Reshape to get real and imaginary parts\n",
    "    real_parts = prediction[0, :5]\n",
    "    imag_parts = prediction[0, 5:]\n",
    "    \n",
    "    # Combine into complex roots\n",
    "    roots = real_parts + 1j * imag_parts\n",
    "    \n",
    "    return roots, real_parts, imag_parts\n",
    "\n",
    "def batch_predict_roots(model, coefficient_list, scaler_X, scaler_y):\n",
    "    \"\"\"Predict roots for multiple polynomials at once\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    coeffs_array = np.array(coefficient_list)\n",
    "    \n",
    "    # Scale the inputs\n",
    "    coeffs_scaled = scaler_X.transform(coeffs_array)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    coeffs_tensor = torch.FloatTensor(coeffs_scaled).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        predictions_scaled = model(coeffs_tensor)\n",
    "    \n",
    "    # Convert back to numpy\n",
    "    predictions_scaled_np = predictions_scaled.cpu().numpy()\n",
    "    \n",
    "    # Inverse scale\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled_np)\n",
    "    \n",
    "    results = []\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        real_parts = prediction[:5]\n",
    "        imag_parts = prediction[5:]\n",
    "        roots = real_parts + 1j * imag_parts\n",
    "        results.append({\n",
    "            'coefficients': coefficient_list[i],\n",
    "            'roots': roots,\n",
    "            'real_parts': real_parts,\n",
    "            'imaginary_parts': imag_parts\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def verify_polynomial_roots(coeffs, roots, tolerance=1e-6):\n",
    "    \"\"\"Verify that the predicted roots actually satisfy the polynomial equation\"\"\"\n",
    "    a, b, c, d, e, f = coeffs\n",
    "    verification_results = []\n",
    "    \n",
    "    for i, root in enumerate(roots):\n",
    "        # Calculate polynomial value at root\n",
    "        poly_value = a*root**5 + b*root**4 + c*root**3 + d*root**2 + e*root + f\n",
    "        error = abs(poly_value)\n",
    "        is_valid = error < tolerance\n",
    "        \n",
    "        verification_results.append({\n",
    "            'root_index': i+1,\n",
    "            'root_value': root,\n",
    "            'polynomial_value': poly_value,\n",
    "            'error': error,\n",
    "            'is_valid_root': is_valid\n",
    "        })\n",
    "    \n",
    "    return verification_results\n",
    "\n",
    "def save_model_components(model, scaler_X, scaler_y, model_path='polynomial_roots_model.pt', \n",
    "                         scaler_x_path='scaler_X.pkl', scaler_y_path='scaler_y.pkl'):\n",
    "    \"\"\"Save the model and scalers for future use\"\"\"\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save scalers\n",
    "    joblib.dump(scaler_X, scaler_x_path)\n",
    "    joblib.dump(scaler_y, scaler_y_path)\n",
    "    \n",
    "    print(\"Model and scalers saved successfully!\")\n",
    "\n",
    "def load_model_components(model_path='polynomial_roots_model.pt',\n",
    "                         scaler_x_path='scaler_X.pkl', scaler_y_path='scaler_y.pkl'):\n",
    "    \"\"\"Load the trained model and scalers for inference\"\"\"\n",
    "    # Initialize model architecture\n",
    "    model = PolynomialRootsModel().to(device)\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # Load scalers\n",
    "    scaler_X = joblib.load(scaler_x_path)\n",
    "    scaler_y = joblib.load(scaler_y_path)\n",
    "    \n",
    "    return model, scaler_X, scaler_y\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    file_path = r\"C:\\Users\\Akshay Patil\\Desktop\\Roots\\R.csv\"  # Update this path as needed\n",
    "    train_loader, val_loader, test_loader, scaler_X, scaler_y = load_and_preprocess_data(file_path)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = PolynomialRootsModel(\n",
    "        input_dim=6, \n",
    "        hidden_dims=[64,128,128,64], \n",
    "        output_dim=10, \n",
    "        cheb_degree=10\n",
    "    ).to(device)\n",
    "    \n",
    "    # Display model architecture\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = train_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        epochs=300, \n",
    "        lr=0.0001, \n",
    "        patience=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    trained_model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_mae = 0.0\n",
    "    criterion = nn.MSELoss()\n",
    "    mae_criterion = nn.L1Loss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = trained_model(inputs)\n",
    "            test_loss += criterion(outputs, targets).item() * inputs.size(0)\n",
    "            test_mae += mae_criterion(outputs, targets).item() * inputs.size(0)\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_mae /= len(test_loader.dataset)\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Test Loss (MSE): {test_loss:.6f}\")\n",
    "    print(f\"Test MAE: {test_mae:.6f}\")\n",
    "    \n",
    "    # Save model components\n",
    "    save_model_components(trained_model, scaler_X, scaler_y)\n",
    "    \n",
    "    # Test prediction function\n",
    "    sample_coeffs = [1, -5, 8, -4, 10, 11]\n",
    "    predicted_roots, real_parts, imag_parts = predict_polynomial_roots(\n",
    "        trained_model, sample_coeffs, scaler_X, scaler_y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSample Prediction:\")\n",
    "    print(f\"Input polynomial coefficients: {sample_coeffs}\")\n",
    "    print(f\"Predicted roots:\")\n",
    "    for i, root in enumerate(predicted_roots):\n",
    "        print(f\"  Root {i+1}: {root:.4f}\")\n",
    "    \n",
    "    # Verify the roots\n",
    "    verification = verify_polynomial_roots(sample_coeffs, predicted_roots)\n",
    "    print(\"\\nRoot verification:\")\n",
    "    for v in verification:\n",
    "        status = \"✓\" if v['is_valid_root'] else \"✗\"\n",
    "        print(f\"  {status} Root {v['root_index']}: Error = {v['error']:.2e}\")\n",
    "    \n",
    "    # Test batch prediction\n",
    "    sample_batch = [\n",
    "        [1, -5, 8, -4, 0, 0],\n",
    "        [2, 1, -3, 0, 1, -2],\n",
    "        [1, 0, 0, 0, 0, -1]\n",
    "    ]\n",
    "    \n",
    "    batch_results = batch_predict_roots(trained_model, sample_batch, scaler_X, scaler_y)\n",
    "    print(\"\\nBatch prediction results:\")\n",
    "    for i, result in enumerate(batch_results):\n",
    "        print(f\"Polynomial {i+1}: {result['coefficients']}\")\n",
    "        print(f\"Roots: {result['roots']}\")\n",
    "        \n",
    "        # Verify the roots\n",
    "        verification = verify_polynomial_roots(result['coefficients'], result['roots'])\n",
    "        print(\"Root verification:\")\n",
    "        for v in verification:\n",
    "            status = \"✓\" if v['is_valid_root'] else \"✗\"\n",
    "            print(f\"  {status} Root {v['root_index']}: Error = {v['error']:.2e}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"Code execution completed successfully!\")\n",
    "    print(\"\\nTo use the trained model in a new session:\")\n",
    "    print(\"1. model, scaler_X, scaler_y = load_model_components()\")\n",
    "    print(\"2. roots, _, _ = predict_polynomial_roots(model, [a,b,c,d,e,f], scaler_X, scaler_y)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
